{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Seminar: Politische Debatten & Polarisierung im Bundestag**\n",
    "## Methoden zur Sentimentanalyse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import numpy as np                 # Numpy\n",
    "import pandas as pd                 #Datafrane\n",
    "\n",
    "# Import Visualization Packages\n",
    "from collections import Counter     # um worte zu zählen\n",
    "import matplotlib.pyplot as plt   # Für Visualisierung\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator #Wordcloud erstellen\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "\n",
    "\n",
    "# Import NLP Packages\n",
    "import nltk\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#  Methode 1: Gibt alle Reden aus dem DataFrame zurück, die den angegebenen Keywords (+Synonymen) entsprechen (nur und Konjunktion)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def find_keywords_with_synonyms(text, keywords):\n",
    "    found_keywords = set()\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            found_keywords.add(word)\n",
    "            synonyms = get_synonyms(word)\n",
    "            found_keywords.update(synonyms)\n",
    "    \n",
    "    return found_keywords\n",
    "\n",
    "def filter_dataframe_by_keywords_with_synonyms(df, keywords):\n",
    "    filtered_df = df[df['text'].apply(lambda x: bool(find_keywords_with_synonyms(x, keywords)))]\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#Methode 2:  Tokenisiert die Sätze in der Spalte 'text' des DataFrames und speichert sie in einem neuen DataFrame.\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "  \n",
    "# Load the appropriate language model\n",
    "\n",
    "import spacy.cli \n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "spacy.load('de_core_news_sm')\n",
    "\n",
    "\n",
    "def tokenize_and_split_sentences(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame in which tokenization should be performed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified DataFrame with tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Tokenize sentences using NLTK\n",
    "    df['tokenized_text'] = df['text'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "    # Create an empty DataFrame with the same columns\n",
    "    columns = ['satz', 'id', 'period', 'date', 'name', 'party', 'redner_id', 'discussion_title']\n",
    "    df_token_satz = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Iterate over each row in the original DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        tokenized_text = row['tokenized_text']\n",
    "        row_dict = row.to_dict()\n",
    "\n",
    "        # Create a DataFrame with tokenized sentences\n",
    "        sentences_df = pd.DataFrame({'satz': tokenized_text})\n",
    "\n",
    "        # Merge the row data with the tokenized sentences DataFrame\n",
    "        merged_df = pd.concat([sentences_df, pd.DataFrame([row_dict] * len(tokenized_text))], axis=1)\n",
    "\n",
    "        # Append the merged DataFrame to the result DataFrame\n",
    "        df_token_satz = pd.concat([df_token_satz, merged_df], ignore_index=True)\n",
    "\n",
    "    return df_token_satz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methode 3: zur Textbereinigung\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def clean_text(df, custom_stopwords=None):\n",
    "    cleaned_sentences = []\n",
    "    cleaned_tokens = []  # New list to store cleaned tokens\n",
    "    \n",
    "    # German stopwords\n",
    "    stopwords_german = set(stopwords.words('german')) - {'nicht'} \n",
    "    \n",
    "    # Update stopwords if custom stopwords are provided\n",
    "    if custom_stopwords:\n",
    "        stopwords_german.update(custom_stopwords)\n",
    "\n",
    "    for sentence in df['satz']:\n",
    "        # Tokenisierung mit Spacy\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.text for token in doc if token.text not in string.punctuation]\n",
    "\n",
    "        # Entfernung von Stoppwörtern mit NLTK\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords_german]\n",
    "        \n",
    "        # Zusammenführen der bereinigten Tokens zu einem Satz\n",
    "        cleaned_sentence = ' '.join(filtered_tokens)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "        \n",
    "        # Store cleaned tokens separately\n",
    "        cleaned_tokens.append(filtered_tokens)\n",
    "\n",
    "    # Assign the cleaned tokens to the DataFrame\n",
    "    df['tokens'] = cleaned_tokens\n",
    "    \n",
    "    # Erstellung einer neuen Spalte 'cleaned_text' im DataFrame mit den bereinigten Sätzen\n",
    "    df['cleaned_text'] = cleaned_sentences\n",
    "\n",
    "    # Delete rows with empty 'cleaned_tokens'\n",
    "    df = df[df['tokens'].map(len) > 0]\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methode 4: Zur Darstellung von nGrammen\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def plot_most_frequent_ngrams(df, num_most_common=10):\n",
    "    # Get the tokens from the DataFrame\n",
    "    tokens = list(df['cleaned_text'].values)\n",
    "\n",
    "    # Count unigrams\n",
    "    unigram_counts = Counter()\n",
    "    for text in tokens:\n",
    "        unigrams = text.split()\n",
    "        unigram_counts.update(unigrams)\n",
    "\n",
    "    # Count bigrams\n",
    "    bigram_counts = Counter()\n",
    "    for text in tokens:\n",
    "        unigrams = text.split()\n",
    "        bigrams = [\",\".join(bigram) for bigram in zip(unigrams[:-1], unigrams[1:])]\n",
    "        bigram_counts.update(bigrams)\n",
    "\n",
    "    # Count trigrams\n",
    "    trigram_counts = Counter()\n",
    "    for text in tokens:\n",
    "        unigrams = text.split()\n",
    "        trigrams = [\",\".join(trigram) for trigram in zip(unigrams[:-2], unigrams[1:-1], unigrams[2:])]\n",
    "        trigram_counts.update(trigrams)\n",
    "\n",
    "    # Get the most frequent tokens\n",
    "    most_common_unigrams = unigram_counts.most_common(num_most_common)\n",
    "    most_common_bigrams = bigram_counts.most_common(num_most_common)\n",
    "    most_common_trigrams = trigram_counts.most_common(num_most_common)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "    # Plot most frequent unigrams\n",
    "    axes[0].barh([str(gram) for gram, count in most_common_unigrams], [count for gram, count in most_common_unigrams])\n",
    "    axes[0].set_title('Most Frequent Unigrams')\n",
    "\n",
    "    # Plot most frequent bigrams\n",
    "    axes[1].barh([str(gram) for gram, count in most_common_bigrams], [count for gram, count in most_common_bigrams])\n",
    "    axes[1].set_title('Most Frequent Bigrams')\n",
    "\n",
    "    # Plot most frequent trigrams\n",
    "    axes[2].barh([str(gram) for gram, count in most_common_trigrams], [count for gram, count in most_common_trigrams])\n",
    "    axes[2].set_title('Most Frequent Trigrams')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\anaconda3\\envs\\env1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Methode 5 zur Modellentwicklung\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def sentiment_analysis(df, text_column):\n",
    "    # Define the sentiment analysis model\n",
    "    nlp_sentiment = pipeline(\"sentiment-analysis\", model='oliverguhr/german-sentiment-bert')\n",
    "\n",
    "    # Apply sentiment analysis to the specified text column in the DataFrame\n",
    "    df['Sentiment'] = df[text_column].apply(lambda x: nlp_sentiment(x))\n",
    "\n",
    "    # Extract sentiment label and score\n",
    "    df['Sentiment_Label'] = [x[0]['label'] for x in df['Sentiment']]\n",
    "    df['Sentiment_Score'] = [x[0]['score'] for x in df['Sentiment']]\n",
    "\n",
    "    # Remove the 'Sentiment' column\n",
    "    df = df.drop(columns=['Sentiment'])\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methoden 6 zur Visualisierung des Sentiments\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_sentiment_analysis(df_grundrechte_original, df_grundrechte_cleaned):\n",
    "    # Count the frequency of each sentiment label\n",
    "    df1_count = df_grundrechte_original['Sentiment_Label'].value_counts()\n",
    "    df2_count = df_grundrechte_cleaned['Sentiment_Label'].value_counts()\n",
    "\n",
    "    # Set the color palette\n",
    "    colors = {'Positive': 'mediumseagreen', 'Negative': 'crimson', 'Neutral': 'royalblue'}\n",
    "\n",
    "    # Create bar plots for sentiment distribution\n",
    "    figure1 = px.bar(x=df1_count.index, y=df1_count.values, color=df1_count.index, color_discrete_map=colors)\n",
    "    figure2 = px.bar(x=df2_count.index, y=df2_count.values, color=df2_count.index, color_discrete_map=colors)\n",
    "\n",
    "    # Customize labels and titles\n",
    "    figure1.update_layout(\n",
    "        title_text='Sentiment Distribution - Original Text',\n",
    "        title_font_size=24,\n",
    "        xaxis_title='Sentiment',\n",
    "        yaxis_title='Count',\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    figure2.update_layout(\n",
    "        title_text='Sentiment Distribution - Cleaned Text',\n",
    "        title_font_size=24,\n",
    "        xaxis_title='Sentiment',\n",
    "        yaxis_title='Count',\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Display the plots\n",
    "    figure1.show()\n",
    "    figure2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methoden 7 zur Visualisierung nach Parteizugehörigkeit\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_sentiment_by_party (df):\n",
    "    # Group the data by party and sentiment label and count the occurrences\n",
    "    party_sentiment = df.groupby(['party', 'Sentiment_Label']).size().reset_index(name='Count')\n",
    "\n",
    "    # Calculate the total count for each party\n",
    "    party_count = party_sentiment.groupby('party')['Count'].sum()\n",
    "\n",
    "    # Calculate the percentage of each sentiment category for each party\n",
    "    party_sentiment['Percentage'] = party_sentiment.apply(lambda row: row['Count'] / party_count[row['party']] * 100, axis=1)\n",
    "\n",
    "    # Create separate dataframes for each sentiment label\n",
    "    positive_df = party_sentiment[party_sentiment['Sentiment_Label'] == 'positive']\n",
    "    negative_df = party_sentiment[party_sentiment['Sentiment_Label'] == 'negative']\n",
    "    neutral_df = party_sentiment[party_sentiment['Sentiment_Label'] == 'neutral']\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=positive_df['party'], y=positive_df['Count'], name='Positive', marker_color='mediumseagreen',\n",
    "                         text=positive_df['Percentage'].apply(lambda x: f'{x:.2f}%'),\n",
    "                         textposition='auto'))\n",
    "    fig.add_trace(go.Bar(x=negative_df['party'], y=negative_df['Count'], name='Negative', marker_color='crimson',\n",
    "                         text=negative_df['Percentage'].apply(lambda x: f'{x:.2f}%'),\n",
    "                         textposition='auto'))\n",
    "    fig.add_trace(go.Bar(x=neutral_df['party'], y=neutral_df['Count'], name='Neutral', marker_color='royalblue',\n",
    "                         text=neutral_df['Percentage'].apply(lambda x: f'{x:.2f}%'),\n",
    "                         textposition='auto'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode='group',\n",
    "        xaxis_title='Partei',\n",
    "        yaxis_title='Anzahl an Sätzen',\n",
    "        title='Sentiment-Verteilung nach Parteizugehörigkeit'\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode 9 zum Visualisierung von Wordclouds nach Sentiment\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sentiment_wordclouds(df):\n",
    "    # Group the data by sentiment label\n",
    "    sentiment_groups = df.groupby('Sentiment_Label')\n",
    "    text_by_sentiment = {}\n",
    "\n",
    "    # Combine the text for each sentiment label\n",
    "    for sentiment, group in sentiment_groups:\n",
    "        text_by_sentiment[sentiment] = ' '.join(group['cleaned_text'].tolist())\n",
    "\n",
    "    # Generate a word cloud for each sentiment\n",
    "    for sentiment, text in text_by_sentiment.items():\n",
    "        wordcloud = WordCloud(background_color='black', width=400, height=300, max_words=150, colormap='tab20c').generate(text)\n",
    "\n",
    "        # Plot the word cloud\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(sentiment + ' Sentiment Word Cloud')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soziologie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
